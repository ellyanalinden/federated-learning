{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
    "\n",
    "https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%206%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0731 14:29:26.089576  6332 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\Vilas_2\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0731 14:29:26.189540  6332 deprecation_wrapper.py:119] From C:\\Users\\Vilas_2\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted\\session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pysyft and define remote workers alice and bob\n",
    "import syft as sy #import Pysyft library\n",
    "hook = sy.TorchHook(torch) #add extra functionalities to support Frderated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\") #define 1st worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\") #2nd worker  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the setting of learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "        \n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading and sending to workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Vilas_2/.pytorch/MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "    datasets.MNIST('~/.pytorch/MNIST_data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob,alice)), #distribute the dataset acroos all workers. It's a Fderated Dataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/.pytorch/MNIST_data/', train=False, download=True,\n",
    "                   transform= transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train and test function\n",
    "For the train fc, because the data batches are distributed across alice and bob, you need to send the model to the right location for each batch. Then, you perform all the operations remotely with the same syntax like doing local pytorch. After done, get back the model updated and the loss to look for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(federated_train_loader): #it's a distributed dataset\n",
    "        model.send(data.location) #send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() #get the new model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test fc doesn't change\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() #sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) #get the index of the max log-propbability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.313616\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.220160\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.068735\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.724446\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 1.033717\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.622254\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.649745\n",
      "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.900343\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.417282\n",
      "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.272216\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.385671\n",
      "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.275365\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.442085\n",
      "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.338076\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.308777\n",
      "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.266436\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.166424\n",
      "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.317791\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.237456\n",
      "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.246858\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.086143\n",
      "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.172519\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.188695\n",
      "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.321731\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.148207\n",
      "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.177772\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.221915\n",
      "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.189713\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.206011\n",
      "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.285656\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.127676\n",
      "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.139881\n",
      "\n",
      "Test set: Average loss: 0.1596, Accuracy: 9517/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.043397\n",
      "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.233103\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.151305\n",
      "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.309083\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.178017\n",
      "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.197325\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.122002\n",
      "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.130944\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.106266\n",
      "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.094502\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.144935\n",
      "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.088885\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.154760\n",
      "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.133177\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.186214\n",
      "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.193167\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.125393\n",
      "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.226258\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.181776\n",
      "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.114153\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.044122\n",
      "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.156076\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.215167\n",
      "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.064831\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.073496\n",
      "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.191161\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.193725\n",
      "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.169298\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.095431\n",
      "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.096426\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.188100\n",
      "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.117333\n",
      "\n",
      "Test set: Average loss: 0.0927, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.101682\n",
      "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.195264\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.081016\n",
      "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.236486\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.192137\n",
      "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.034543\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.178457\n",
      "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.142010\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.155047\n",
      "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.037238\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.148765\n",
      "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.023657\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.048494\n",
      "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.250334\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.101539\n",
      "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.095908\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.094698\n",
      "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.021230\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.022090\n",
      "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.088488\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.089577\n",
      "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.139973\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.043461\n",
      "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.060252\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.031674\n",
      "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.083389\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.033763\n",
      "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.048149\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.099184\n",
      "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.099377\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.020806\n",
      "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.034633\n",
      "\n",
      "Test set: Average loss: 0.0701, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.065408\n",
      "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.047307\n",
      "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.121810\n",
      "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.049595\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.058868\n",
      "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.045612\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.036585\n",
      "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.141508\n",
      "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.105457\n",
      "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.084639\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.107272\n",
      "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.105090\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.029084\n",
      "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.032539\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.035844\n",
      "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.113972\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.046937\n",
      "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.071952\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.116033\n",
      "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.037878\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.085651\n",
      "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.047537\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.186456\n",
      "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.048377\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.127646\n",
      "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.046011\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.095152\n",
      "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.032375\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.095437\n",
      "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.060701\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.111463\n",
      "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.048203\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.026946\n",
      "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.022932\n",
      "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.029795\n",
      "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.029099\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.113499\n",
      "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.008633\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.013274\n",
      "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.058986\n",
      "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.033684\n",
      "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.014692\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.022053\n",
      "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.103131\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.055069\n",
      "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.010987\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.016399\n",
      "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.072738\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.035310\n",
      "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.025894\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.021788\n",
      "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.029221\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.030575\n",
      "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.034510\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.046491\n",
      "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.074246\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.006990\n",
      "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.108876\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.016194\n",
      "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.040286\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.019280\n",
      "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.049013\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.017557\n",
      "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.003885\n",
      "\n",
      "Test set: Average loss: 0.0565, Accuracy: 9824/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.105805\n",
      "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.039563\n",
      "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.028100\n",
      "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.111757\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.012572\n",
      "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.020595\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.007196\n",
      "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.164978\n",
      "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.030457\n",
      "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.004209\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.082345\n",
      "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.040032\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.041765\n",
      "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.017844\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.010150\n",
      "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.059283\n",
      "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.032701\n",
      "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.012541\n",
      "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.044780\n",
      "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.058152\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.044548\n",
      "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.165212\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.078399\n",
      "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.054892\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.018870\n",
      "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.064283\n",
      "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.002047\n",
      "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.014063\n",
      "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.125286\n",
      "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.033924\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.089056\n",
      "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.025620\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.056609\n",
      "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.015395\n",
      "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.111187\n",
      "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.163147\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.086746\n",
      "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.012479\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.155883\n",
      "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.024833\n",
      "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.106731\n",
      "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.045211\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.035382\n",
      "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.005286\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.149087\n",
      "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.027592\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.051070\n",
      "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.005588\n",
      "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.042297\n",
      "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.068295\n",
      "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.048791\n",
      "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.036227\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.007266\n",
      "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.019249\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.099323\n",
      "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.010475\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.047018\n",
      "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.012271\n",
      "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.012015\n",
      "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.057611\n",
      "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.067238\n",
      "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.019195\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.035003\n",
      "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.052756\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.066279\n",
      "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.065714\n",
      "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.015069\n",
      "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.023398\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.005099\n",
      "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.010035\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.023687\n",
      "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.010381\n",
      "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.016318\n",
      "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.091487\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.004799\n",
      "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.046071\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.009292\n",
      "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.004813\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.112734\n",
      "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.022332\n",
      "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.024887\n",
      "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.030130\n",
      "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.039746\n",
      "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.011101\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.103627\n",
      "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.019940\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.019821\n",
      "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.045551\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.058305\n",
      "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.004210\n",
      "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.036830\n",
      "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.081381\n",
      "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.065795\n",
      "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.055635\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.058719\n",
      "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.066525\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.010319\n",
      "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.040560\n",
      "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.015400\n",
      "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.007958\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.013275\n",
      "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.017370\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.032925\n",
      "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.029947\n",
      "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.044707\n",
      "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.034771\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.016970\n",
      "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.048321\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.238707\n",
      "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.012185\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.034589\n",
      "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.017281\n",
      "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.028472\n",
      "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.020224\n",
      "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.010615\n",
      "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.038214\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.002532\n",
      "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.012678\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.107807\n",
      "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.054905\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.117468\n",
      "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.065394\n",
      "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.011707\n",
      "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.197588\n",
      "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.096303\n",
      "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.008557\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.164265\n",
      "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.002940\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.077714\n",
      "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.026916\n",
      "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.006302\n",
      "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.020801\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.065909\n",
      "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.006089\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.011678\n",
      "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.073905\n",
      "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.022354\n",
      "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.013260\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.009728\n",
      "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.012601\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.018364\n",
      "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.059188\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.008359\n",
      "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.027821\n",
      "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.015833\n",
      "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.099757\n",
      "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.029392\n",
      "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.105846\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.004465\n",
      "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.034632\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.022677\n",
      "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.017077\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.011182\n",
      "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.003732\n",
      "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.009205\n",
      "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.033501\n",
      "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.020773\n",
      "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003108\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.004842\n",
      "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.009594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0430, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Wall time: 42min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) #momentum not supported\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if(args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
